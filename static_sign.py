# -*- coding: utf-8 -*-
"""sign.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FQTe1DXTC8lJWVPF8VTMC7xfws_Uz-K9
"""

import pandas as pd
from keras.models import Sequential
from keras.layers import ConvLSTM2D, MaxPooling3D, Flatten, Dense, Dropout
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import ReduceLROnPlateau

# Load dataset
train_df = pd.read_csv('sign_mnist_train.csv')
test_df = pd.read_csv('sign_mnist_test.csv')

# Separate features and labels
X_train = train_df.iloc[:, 1:].values  # Pixel values
y_train = train_df.iloc[:, 0].values  # Labels

X_test = test_df.iloc[:, 1:].values
y_test = test_df.iloc[:, 0].values

# Normalize pixel values
X_train = X_train / 255.0
X_test = X_test / 255.0

# Reshape data to (samples, time_steps, height, width, channels)
X_train = X_train.reshape(-1, 1, 28, 28, 1)
X_test = X_test.reshape(-1, 1, 28, 28, 1)

# One-hot encode labels
y_train = to_categorical(y_train, num_classes=25)  # Adjust num_classes as needed
y_test = to_categorical(y_test, num_classes=25)

# Train-validation split
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Define CNN-LSTM model
model = Sequential([
    ConvLSTM2D(
        filters=32,
        kernel_size=(3, 3),
        activation='relu',
        input_shape=(1, 28, 28, 1),
        return_sequences=True
    ),
    Dropout(0.2),
    MaxPooling3D(pool_size=(1, 2, 2)),

    ConvLSTM2D(
        filters=64,
        kernel_size=(3, 3),
        activation='relu',
        return_sequences=True
    ),
    Dropout(0.3),
    MaxPooling3D(pool_size=(1, 2, 2)),

    ConvLSTM2D(
        filters=128,
        kernel_size=(3, 3),
        activation='relu',
        return_sequences=False
    ),
    Dropout(0.4),

    Flatten(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(25, activation='softmax')  # Adjust output size based on the number of classes
])

# Compile model
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)

# Train the model
history = model.fit(
    X_train,
    y_train,
    epochs=20,
    batch_size=8,
    validation_data=(X_val, y_val),
    shuffle=True,
    callbacks=[reduce_lr]
)

# Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test, batch_size=4)
print(f"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}")

import matplotlib.pyplot as plt
plt.figure(figsize=(12, 5))

# Plot loss
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# Plot accuracy
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt



# Plot training and validation accuracy
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')

# Add test accuracy as a horizontal line
plt.axhline(y=test_accuracy, color='r', linestyle='--', label=f'Test Accuracy: {test_accuracy:.4f}')

plt.title('Training, Validation, and Test Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

y_pred_ann=model.predict(X_test)
y_test_labels = np.argmax(y_test, axis=1)
y_pred_labels = np.argmax(y_pred_ann, axis=1)

# Plot a sample of test vs. predicted values
plt.figure(figsize=(12, 6))
plt.plot(y_test_labels[:225], label='True Labels', marker='o', linestyle='dashed')
plt.plot(y_pred_labels[:225], label='Predicted Labels', marker='x', linestyle='dotted')

plt.title('Comparison of True vs. Predicted Labels')
plt.xlabel('Sample Index')
plt.ylabel('Class Label')
plt.legend()

plt.show()

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

# Convert one-hot encoded labels back to class indices
y_test_labels = np.argmax(y_test, axis=1)
y_pred_labels = np.argmax(y_pred_ann, axis=1)

# Count occurrences of each label
true_counts = Counter(y_test_labels)
pred_counts = Counter(y_pred_labels)

# Get unique class labels
classes = sorted(set(y_test_labels))

# Create a bar plot
plt.figure(figsize=(12, 6))
plt.bar(classes, [true_counts[c] for c in classes], alpha=0.6, label="True Labels")
plt.bar(classes, [pred_counts[c] for c in classes], alpha=0.6, label="Predicted Labels")

plt.xlabel("Class Label")
plt.ylabel("Count")
plt.title("Comparison of True vs. Predicted Labels")
plt.legend()
plt.show()

from sklearn.metrics import confusion_matrix, classification_report
y_pred_ann = np.argmax(y_pred_ann, axis=1)  # Converts probabilities to class labels
y_test = np.argmax(y_test, axis=1)  # Ensure y_test is also in label form

# Assuming y_test contains true labels and y_pred_ann contains predicted labels
cm = confusion_matrix(y_test, y_pred_ann)

# Plot confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(26), yticklabels=range(26))
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

print("Classification Report:")
print(classification_report(y_test, y_pred_ann))

alphabet_labels = [chr(i) for i in range(65, 91) if chr(i) not in ['J', 'Z']]  # ['A', 'B', ..., 'I', 'K', ..., 'Y']


index = np.random.randint(0, X_test.shape[0])
sample_image = X_test[index]
sample_label = y_test[index]

sample_image_reshaped = sample_image.reshape(1, 1, 28, 28, 1)

prediction = model.predict(sample_image_reshaped)
predicted_label = np.argmax(prediction)


actual_alphabet = alphabet_labels[sample_label]
predicted_alphabet = alphabet_labels[predicted_label]


plt.imshow(sample_image.squeeze(), cmap='gray')
plt.title(f"Actual: {actual_alphabet}, Predicted: {predicted_alphabet}")
plt.axis('off')
plt.show()

import cv2
import numpy as np
from keras.models import load_model

model = load_model('sign_lstm_model.h5')

alphabet_labels = [chr(i) for i in range(65, 91) if chr(i) != 'J']
# A‚ÄìI, K‚ÄìZ ‚Üí 25 letters total

import os

video_path = '/content/WIN_20250419_22_24_08_Pro.jpg'
print("File exists:", os.path.exists(video_path))
cap = cv2.VideoCapture(video_path)

cap = cv2.VideoCapture(video_path)
if not cap.isOpened():
    print("‚ùå Failed to open video")
else:
    print("‚úÖ Video opened successfully")
    print("Total frames:", cap.get(cv2.CAP_PROP_FRAME_COUNT))
    print("FPS:", cap.get(cv2.CAP_PROP_FPS))
    print("Width x Height:", cap.get(cv2.CAP_PROP_FRAME_WIDTH), "x", cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

frames = []

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    try:
        # Resize to 28x28 and convert to grayscale
        frame_resized = cv2.resize(frame, (28, 28))
        frame_gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)
        gray = cv2.equalizeHist(frame_gray)
        _, thresh = cv2.threshold(frame_gray, 100, 255, cv2.THRESH_BINARY)
        # Resize to match dataset size
        resized = cv2.resize(thresh, (28, 28))
        frame_normalized = frame_gray / 255.0

        # Reshape to (28, 28, 1)
        reshaped = frame_normalized.reshape(28, 28, 1)
        frames.append(reshaped)
    except Exception as e:
        print("‚ö†Ô∏è Frame processing error:", e)

cap.release()

# Convert to numpy array if frames were collected
if len(frames) == 0:
    print("‚ùå No frames were extracted. Check the video file.")
else:
    print("‚úÖ Total frames extracted:", len(frames))

    # Shape: (num_frames, 28, 28, 1)
    frames_np = np.array(frames)
    print("üìê Frames shape before prediction:", frames_np.shape)

predicted_alphabets = []

for i, frame in enumerate(frames):
    frame_input = frame.reshape(1, 1, 28, 28, 1)  # Add batch and time dimension
    prediction = model.predict(frame_input)
    label = np.argmax(prediction)

    if label < len(alphabet_labels):
        predicted_alphabets.append(alphabet_labels[label])
    else:
        predicted_alphabets.append("?")

# Print predictions
print("\nüî§ Frame-wise Predictions:")
for i, letter in enumerate(predicted_alphabets):
    print(f"Frame {i+1}: Predicted Letter: {letter}")

from collections import Counter

# Get most common prediction across frames
final_prediction = Counter(predicted_alphabets).most_common(1)[0][0]
print("\nüéØ Final Predicted Sign:", final_prediction)

cap = cv2.VideoCapture(video_path)
if not cap.isOpened():
    raise RuntimeError("‚ùå Failed to open video. Check the path.")

width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS)

output_path = "final_prediction_overlay.mp4"
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Draw final prediction on the frame
    cv2.putText(frame, f'Prediction: {final_prediction}', (10, 40),
                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 3)

    out.write(frame)

cap.release()
out.release()

# === Step 4: Download final video ===
if os.path.exists(output_path):
    print("‚úÖ Final prediction video ready. Downloading...")
    files.download(output_path)
else:
    print("‚ùå Output video not found.")

