# -*- coding: utf-8 -*-
"""Yet another copy of notebook8e24453a4e

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_Dv5LtSbqkFFk_p6sXaw1l_OMi2gZt9s
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
risangbaskoro_wlasl_processed_path = kagglehub.dataset_download('risangbaskoro/wlasl-processed')

print('Data source import complete.')



import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import cv2
import json
import os
from sklearn.model_selection import train_test_split
from keras.utils import to_categorical
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv3D, MaxPooling3D

# Define the fixed size for the video frames
img_size = (64, 64)
num_frames = 30
num_classes = 2000

# main_path = kagglehub.dataset_download('risangbaskoro/wlasl-processed')

print(risangbaskoro_wlasl_processed_path)

main_path = '/kaggle/input/wlasl-processed/'

data_df = pd.read_json(main_path + 'WLASL_v0.3.json')

data_df.head()

data_df.shape

import shutil
import os

main_path = '/kaggle/input/wlasl-processedvideos'
video_name = '17726.mp4'
source_path = os.path.join(main_path, video_name)
destination_path = f'./{video_name}'  # Save it in the current working directory

# Copy the file
if os.path.exists(source_path):
    shutil.copy(source_path, destination_path)
    print(f"‚úÖ Video copied to: {destination_path}")
else:
    print("‚ùå Video file not found.")

import pandas as pd
pd.set_option('display.max_colwidth', None)

row = data_df[data_df['gloss'] == 'thin']
print(row)

def get_videos_ids(json_list):
    """
    function to check if the video id is available in the dataset
    and return the viedos ids of the current instance

    input: instance json list
    output: list of videos_ids

    """
    videos_list = []
    for ins in json_list:
        video_id = ins['video_id']
        if os.path.exists(f'{main_path}videos/{video_id}.mp4'):
            videos_list.append(video_id)
    return videos_list

def get_json_features(json_list):
    """
    function to check if the video id is available in the dataset
    and return the viedos ids and url or any other featrue of the current instance

    input: instance json list
    output: list of videos_ids

    """
    videos_ids = []
    videos_urls = []
    for ins in json_list:
        video_id = ins['video_id']
        video_url = ins['url']
        if os.path.exists(f'{main_path}videos/{video_id}.mp4'):
            videos_ids.append(video_id)
            videos_urls.append(video_url)
    return videos_ids, videos_urls

with open(main_path+'WLASL_v0.3.json', 'r') as data_file:
    json_data = data_file.read()

instance_json = json.loads(json_data)

get_videos_ids(instance_json[0]['instances'])[0]

len(get_videos_ids(instance_json[0]['instances']))

data_df['videos_ids'] = data_df['instances'].apply(get_videos_ids)

features_df = pd.DataFrame(columns=['gloss', 'video_id', 'url'])
for row in data_df.iterrows():
    ids, urls = get_json_features(row[1][1])
    word = [row[1][0]] * len(ids)
    df = pd.DataFrame(list(zip(word, ids, urls)), columns = features_df.columns)
    features_df = pd.concat([features_df, df], ignore_index=True)

features_df.index.name = 'index'
features_df

import numpy as np
import cv2
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from keras.utils import to_categorical
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Masking, Bidirectional, GRU, Dense, Dropout
from tensorflow.keras.preprocessing.sequence import pad_sequences
import os

img_size = (128, 128)
num_frames = 10
num_classes = features_df['gloss'].nunique()

print(num_classes)

gloss_to_idx = {gloss: i for i, gloss in enumerate(sorted(features_df['gloss'].unique()))}

# Load pre-trained CNN
base_cnn = MobileNetV2(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))
cnn_model = Model(inputs=base_cnn.input, outputs=base_cnn.output)

pip uninstall mediapipe -y

pip install mediapipe==0.10.9

import mediapipe as mp

print(mp.__version__)

# Initialize MediaPipe holistic model (for full body, hands, face keypoints)
mp_holistic = mp.solutions.holistic
mp_drawing = mp.solutions.drawing_utils

def extract_keypoints(frame):
    with mp_holistic.Holistic(static_image_mode=False,model_complexity=1,enable_segmentation=False,refine_face_landmarks=False) as holistic:
        results = holistic.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

        # Extract pose (33), left_hand (21), right_hand (21), face (optional, 468)
        pose = results.pose_landmarks.landmark if results.pose_landmarks else []
        left_hand = results.left_hand_landmarks.landmark if results.left_hand_landmarks else []
        right_hand = results.right_hand_landmarks.landmark if results.right_hand_landmarks else []

        # Combine all landmarks into one vector
        keypoints = []
        for lm_list in [pose, left_hand, right_hand]:
            for lm in lm_list:
                keypoints.extend([lm.x, lm.y, lm.z])

        # Pad if not enough landmarks (for consistency)
        while len(keypoints) < 1000:
            keypoints.append(0.0)

        return np.array(keypoints[:1000])  # Truncate extra just in case

def extract_frame_features(url):
    try:
        cap = cv2.VideoCapture(url)
        if not cap.isOpened():
            print("Failed to open video:", url)
            return None
        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        step = max(1, total // num_frames)
        features = []

        for i in range(0, total, step):
            cap.set(cv2.CAP_PROP_POS_FRAMES, i)
            ret, frame = cap.read()
            if not ret:
                break
            frame = cv2.resize(frame, img_size)
            keypoints = extract_keypoints(frame)
            features.append(keypoints)
            if len(features) == num_frames:
                break

        cap.release()
        return np.array(features)
    except Exception as e:
        print(f"Error processing video: {e}")
        return None

import os
import numpy as np
from tqdm import tqdm

features_df_part3 = features_df[7986:]

X, y = [], []

# Optional: resume if partially saved
if os.path.exists("X_partial.npy") and os.path.exists("y_partial.npy"):
    X = list(np.load("X_partial.npy", allow_pickle=True))
    y = list(np.load("y_partial.npy", allow_pickle=True))
    start_idx = len(X)
    print(f"‚úÖ Resuming from index {start_idx}")
else:
    start_idx = 0

# Main loop
for idx, (_, row) in enumerate(tqdm(features_df_part3.iterrows(), total=len(features_df_part3))):
    if idx < start_idx:
        continue  # Skip already processed

    local_path = os.path.join(main_path, "videos", f"{row['video_id']}.mp4")
    video_path = local_path if os.path.exists(local_path) else row['url']

    features = extract_frame_features(video_path)
    if features is not None and len(features) > 0:
        X.append(features)
        y.append(gloss_to_idx[row['gloss']])

    # Save every 100 samples
    if (idx + 1) % 100 == 0:
        np.save("X_partial.npy", np.array(X, dtype=object))
        np.save("y_partial.npy", np.array(y))
        print(f"üíæ Saved progress at index {idx + 1} ‚Äî Total so far: {len(X)}")

# Final save
np.save("X_final.npy", np.array(X, dtype=object))
np.save("y_final.npy", np.array(y))
print("‚úÖ All features extracted and saved.")



import numpy as np

# Load the saved files
X = np.load("X_end.npy", allow_pickle=True)
y = np.load("y_end.npy", allow_pickle=True)

print("X shape:", X.shape, "| dtype:", X.dtype)
print("y shape:", y.shape, "| dtype:", y.dtype)

import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Bidirectional, GRU, Dense, Dropout, Masking
from tensorflow.keras.optimizers import Adam

X = np.array(X, dtype=np.float32)

print("X shape:", X.shape)  # Should print something like (11979, frames, keypoints)
print("y shape:", y.shape)  # Should print (11979,)

import numpy as np
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# --- Example input ---
# X: numpy array of shape (samples, frames, features)
# y: list or array of class labels

# Let's assume you already have these:
# X = np.array([...])
# y = np.array([...])

# --- Step 1: Filter to top 10 most common classes ---
top_classes = [cls for cls, _ in Counter(y).most_common(10)]
mask = np.isin(y, top_classes)

X_filtered = X[mask]
y_filtered = y[mask]

# Normalize each sample: (samples, frames, features)
X_filtered = (X_filtered - np.mean(X_filtered, axis=(1, 2), keepdims=True)) / np.std(X_filtered, axis=(1, 2), keepdims=True)

# Encode class labels to 0-9
le = LabelEncoder()
y_encoded = le.fit_transform(y_filtered)

# --- Step 2: Augmentation functions ---

def jitter_landmarks(sample, sigma=0.01):
    noise = np.random.normal(loc=0.0, scale=sigma, size=sample.shape)
    return sample + noise

def frame_dropout(sample, drop_rate=0.1):
    keep_mask = np.random.rand(sample.shape[0]) > drop_rate
    kept_frames = sample[keep_mask]
    # Pad if frames were dropped
    if kept_frames.shape[0] < sample.shape[0]:
        pad_count = sample.shape[0] - kept_frames.shape[0]
        pad_frames = np.tile(kept_frames[-1:], (pad_count, 1))  # repeat last frame
        kept_frames = np.vstack([kept_frames, pad_frames])
    return kept_frames



def normalize_sequence(seq):
    return (seq - np.mean(seq)) / (np.std(seq) + 1e-8)


# Apply augmentation
X_augmented = []
y_augmented = []

for xi, yi in zip(X_filtered, y_encoded):
    # Jittered version
    X_augmented.append(normalize_sequence(jitter_landmarks(xi)))
    y_augmented.append(yi)

    # Frame dropout version
    X_augmented.append(frame_dropout(xi))
    y_augmented.append(yi)

    #  # Frame dropout version
    # X_augmented.append(time_warp(xi))
    # y_augmented.append(yi)

    #  # Frame dropout version
    # X_augmented.append(scale_sequence(xi))
    # y_augmented.append(yi)



# Combine with original
X_combined = np.concatenate([X_filtered, np.array(X_augmented)], axis=0)
y_combined = np.concatenate([y_encoded, np.array(y_augmented)], axis=0)

# --- Step 3: Train-test split ---
X_train, X_test, y_train, y_test = train_test_split(
    X_combined, y_combined, test_size=0.2, stratify=y_combined, random_state=42
)

print("Shapes:")
print("X_train:", X_train.shape)
print("y_train:", y_train.shape)
print("X_test:", X_test.shape)
print("y_test:", y_test.shape)

from tensorflow.keras.layers import Layer
import tensorflow.keras.backend as K

class Attention(Layer):
    def __init__(self):
        super(Attention, self).__init__()

    def build(self, input_shape):
        self.W = self.add_weight(shape=(input_shape[-1], 1), initializer='normal', trainable=True)

    def call(self, x):
        e = K.tanh(K.dot(x, self.W))  # shape: (batch, time, 1)
        a = K.softmax(e, axis=1)      # attention weights
        output = K.sum(x * a, axis=1) # context vector
        return output

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input,Bidirectional, GRU, Dense, Dropout, Masking
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import BatchNormalization, Concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Attention, GlobalAveragePooling1D, LayerNormalization
from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization
# Input shape and classes
input_shape = (10, 1000)
num_classes = 10  # Or your actual number of classes

# One-hot encode the labels
y_train_one_hot = to_categorical(y_train, num_classes=num_classes)
y_test_one_hot = to_categorical(y_test, num_classes=num_classes)

# Model definition
inputs = Input(shape=input_shape)
x = Masking(mask_value=0.0)(inputs)
x = Bidirectional(GRU(128, return_sequences=True))(x)
x = BatchNormalization()(x)
x = Dropout(0.3)(x)
x = Bidirectional(GRU(64, return_sequences=True))(x)
x = Dropout(0.3)(x)

# Apply Attention ‚Äî use the same tensor as query, key, and value
attn_output = MultiHeadAttention(num_heads=2, key_dim=64)(x, x)
x = LayerNormalization()(x + attn_output)  # residual connection
x = GlobalAveragePooling1D()(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.4)(x)
outputs = Dense(num_classes, activation='softmax')(x)

model = Model(inputs, outputs)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

from sklearn.utils.class_weight import compute_class_weight
class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weights_dict = dict(enumerate(class_weights))

from tensorflow.keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.callbacks import EarlyStopping

lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, min_lr=1e-6)
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

history = model.fit(
    X_train, y_train_one_hot,
    validation_data=(X_test, y_test_one_hot),
    epochs=50,
    batch_size=64,
    class_weight=class_weights_dict,
    callbacks=[lr_schedule]
)


# Evaluate the model on validation data
loss, acc = model.evaluate(X_test, y_test_one_hot)
print(f"Val Accuracy: {acc:.4f}")

from sklearn.metrics import classification_report
y_pred = model.predict(X_test)
print(classification_report(np.argmax(y_test_one_hot, axis=1), np.argmax(y_pred, axis=1)))

cm=confusion_matrix(np.argmax(y_test_one_hot, axis=1), np.argmax(y_pred, axis=1))
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(9), yticklabels=range(9))
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

model.save('asl.keras')



import matplotlib.pyplot as plt
plt.figure(figsize=(12, 5))

# Plot loss
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# Plot accuracy
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

signs = ['Before', 'Candy', 'Computer', 'Cool', 'Cousin', 'Drink', 'Go', 'Help','Thin','You']

import cv2
import numpy as np
import mediapipe as mp
from tensorflow.keras.models import load_model
from google.colab import files

modelt = load_model('asl.keras')

# ‚¨ÖÔ∏è change this to your video
output_path = "output_prediction.mp4"
SEQUENCE_LENGTH = 10
INPUT_SIZE = 1000
IMG_SIZE = (224, 224)

video_path = '/content/WIN_20250420_14_42_52_Pro.mp4'  # ‚Üê change this to your actual video file
cap = cv2.VideoCapture(video_path)

# === MediaPipe Holistic Setup ===
mp_holistic = mp.solutions.holistic
mp_drawing = mp.solutions.drawing_utils

def extract_keypoints(frame):
    with mp_holistic.Holistic(static_image_mode=False,
                               model_complexity=1,
                               enable_segmentation=False,
                               refine_face_landmarks=False) as holistic:
        results = holistic.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

        keypoints = []
        for part in [results.pose_landmarks,
                     results.left_hand_landmarks,
                     results.right_hand_landmarks]:
            if part:
                keypoints.extend([coord for lm in part.landmark for coord in (lm.x, lm.y, lm.z)])
        while len(keypoints) < INPUT_SIZE:
            keypoints.append(0.0)
        return np.array(keypoints[:INPUT_SIZE])

from collections import Counter

# === Video Reading ===
cap = cv2.VideoCapture(video_path)
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS)

fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

sequence = []
predicted_letters = []
last_frame = None  # Store the last processed frame

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Resize for keypoint extraction
    resized = cv2.resize(frame, IMG_SIZE)
    keypoints = extract_keypoints(resized)

    sequence.append(keypoints)
    if len(sequence) > SEQUENCE_LENGTH:
        sequence.pop(0)

    if len(sequence) == SEQUENCE_LENGTH:
        input_data = np.expand_dims(np.array(sequence), axis=0)  # (1, 10, 1000)
        prediction = modelt.predict(input_data, verbose=0)
        pred = modelt.predict(input_data)
        top_indices = np.argsort(pred[0])[::-1][:]  # top predictions

        for i in top_indices:
            print(f"{signs[i]} (class {i}) ‚ûú {pred[0][i]:.4f}")

        pred_idx = np.argmax(prediction)
        pred_class = signs[pred_idx]
        confidence = np.max(prediction)

        predicted_letters.append(pred_class)

        # Draw prediction on video frame
        cv2.putText(frame, f'{pred_class} ({confidence:.2f})', (10, 40),
                    cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 3)

    # Keep track of last frame
    last_frame = frame.copy()
    out.write(frame)

cap.release()

# === Add final prediction to the last frame and write it again ===
if predicted_letters:
    most_common_letter, count = Counter(predicted_letters).most_common(1)[0]
    print("üìù Final Predicted Letter (Most Frequent):", most_common_letter)

    if last_frame is not None:
        cv2.putText(last_frame, f'Final Prediction: {most_common_letter}', (10, height - 40),
                    cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 0, 0), 3)
        out.write(last_frame)  # Add the final frame again with final result
else:
    print("‚ö†Ô∏è No predictions were made.")

out.release()

print("‚úÖ Prediction complete. Download below:")
files.download(output_path)